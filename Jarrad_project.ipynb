{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnIbwiK7Ohv2",
    "outputId": "323b604b-5c49-4b2c-cf17-b06f09ffc176",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:11:50.717048: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 22:11:50.985069: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-24 22:11:52.441873: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jarrad_foley135/miniconda3/envs/mlis_cw_car/lib/\n",
      "2023-03-24 22:11:52.442030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jarrad_foley135/miniconda3/envs/mlis_cw_car/lib/\n",
      "2023-03-24 22:11:52.442038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from '/home/jarrad_foley135/miniconda3/envs/mlis_cw_car/lib/python3.9/site-packages/tensorflow/_api/v2/version/__init__.py'>\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:11:54.349256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:54.376143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:54.376187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, callbacks\n",
    "  \n",
    "print(tf.version)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "#import datetime, os\n",
    "\n",
    "from keras.applications import VGG16, VGG19, ResNet50, InceptionV3, InceptionResNetV2, Xception, MobileNet, DenseNet121, \\\n",
    "    NASNetMobile, EfficientNetB0, MobileNetV2, MobileNetV3Large\n",
    "from tensorflow.python.keras.layers import Conv2D, Flatten, Dropout, Dense, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "49wbEaM1PCCR",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_id   angle  speed   filename\n",
      "0          7105  0.5000    1.0   7105.png\n",
      "1         16298  0.5000    0.0  16298.png\n",
      "2          9422  0.4375    0.0   9422.png\n",
      "3          4614  0.5625    1.0   4614.png\n",
      "4          1076  0.6250    1.0   1076.png\n",
      "...         ...     ...    ...        ...\n",
      "14821     11628  0.5625    1.0  11628.png\n",
      "14822       723  0.6250    1.0    723.png\n",
      "14823      4078  0.6875    1.0   4078.png\n",
      "14824      6294  0.5000    1.0   6294.png\n",
      "14825      5939  0.6250    1.0   5939.png\n",
      "\n",
      "[14826 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "img_size = [240, 320]\n",
    "\n",
    "df = pd.read_csv(r'machine-learning-in-science-ii-2023/training_norm.csv')\n",
    "df['filename'] = df[\"image_id\"].astype(str) + \".png\"\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "# new_df = pd.read_csv(r'machine-learning-in-science-ii-2023/new_training_norm.csv')\n",
    "# new_df['filename'] = new_df[\"image_id\"].astype(str) + \"_speed-\" + new_df[\"speed\"].astype(str) + \"_angle-\" + new_df[\"angle\"].astype(str) + \".png\"\n",
    "# #new_df['filename'] = new_df[\"image_id\"].astype(str) + \".png\"\n",
    "\n",
    "#print(new_df)\n",
    "\n",
    "x_train = df[0:int(len(df) * 0.6)]\n",
    "x_validate = df[int(len(df) * 0.6):int(len(df) * 0.8)]\n",
    "x_evaluate = df[int(len(df) * 0.8):]\n",
    "\n",
    "# new_x_train = new_df[0:int(len(new_df) * 0.6)]\n",
    "# new_x_validate = new_df[int(len(new_df) * 0.6):int(len(new_df) * 0.8)]\n",
    "# new_x_evaluate = new_df[int(len(new_df) * 0.8):]\n",
    "\n",
    "# x_train = df[0:int(len(df) * 0.8)]\n",
    "# x_validate = df[int(len(df) * 0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8895 validated image filenames.\n",
      "Found 2965 validated image filenames.\n",
      "Found 2966 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shift = 0.1\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=shift,\n",
    "    height_shift_range=shift\n",
    ")\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "    dataframe=x_train,\n",
    "    directory=\"machine-learning-in-science-ii-2023/training_data/training_data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=[\"angle\",\"speed\"],\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode='other')\n",
    "\n",
    "#print(train_generator)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=x_validate,\n",
    "    directory=\"machine-learning-in-science-ii-2023/training_data/training_data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=[\"angle\",\"speed\"],\n",
    "    target_size=img_size,\n",
    "    shuffle=False,\n",
    "    class_mode='other')\n",
    "\n",
    "evaluate_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "eval_generator = evaluate_datagen.flow_from_dataframe(\n",
    "    dataframe=x_evaluate,\n",
    "    directory=\"machine-learning-in-science-ii-2023/training_data/training_data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=[\"angle\",\"speed\"],\n",
    "    target_size=img_size,\n",
    "    shuffle=False,\n",
    "    class_mode='other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ibuJZqAXQrWJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:11:55.138537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 22:11:55.147410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:55.147470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:55.147509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:56.320910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:56.321409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:56.321429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-24 22:11:56.321465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-24 22:11:56.321506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5413 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "shape = (*img_size, 3) # inherited image size with 3 color filters\n",
    "input_shape = [240, 320, 3]\n",
    "transfer = MobileNetV2(\n",
    "    input_shape=input_shape, \n",
    "    include_top=False, \n",
    "    weights='imagenet',\n",
    "    classifier_activation=\"relu\"\n",
    ")\n",
    "\n",
    "for layer in transfer.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#transfer.trainable = False # freeze the first layers to the imagenet weights\n",
    "\n",
    "# CNN base\n",
    "model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=shape))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(transfer)\n",
    "model.build()\n",
    "        \n",
    "model.add(layers.Conv2D(1280, (3, 3), strides= (2,2), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "#model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten()),\n",
    "\n",
    "# model.add(layers.Dense(256, activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "# model.add(layers.Dense(16, activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2,  activation='relu', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEzHX-7ESeCl",
    "outputId": "b1ad1ed9-87ef-4a37-b392-59e9ebb807d3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 8, 10, 1280)      2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 3, 4, 1280)        14746880  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 3, 4, 1280)       5120      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 15360)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                983104    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,995,618\n",
      "Trainable params: 15,737,442\n",
      "Non-trainable params: 2,258,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5loIug93TW1E",
    "outputId": "f470435d-196d-435b-cb96-39fc27d0552f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:12:02.623928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-24 22:12:11.029394: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2023-03-24 22:12:18.963688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-03-24 22:12:19.101340: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f6e979144a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-24 22:12:19.101422: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2023-03-24 22:12:19.273821: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-24 22:12:20.054074: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2023-03-24 22:12:20.131737: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-03-24 22:12:22.942158: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.82GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-03-24 22:12:22.942511: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.82GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/556 [==========>...................] - ETA: 1:07 - loss: 0.1407"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:13:06.963978: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-03-24 22:13:06.964036: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 146s 218ms/step - loss: 0.0917 - val_loss: 0.0261\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 125s 225ms/step - loss: 0.0410 - val_loss: 0.0179\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 118s 212ms/step - loss: 0.0273 - val_loss: 0.0162\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 123s 220ms/step - loss: 0.0230 - val_loss: 0.0137\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 122s 220ms/step - loss: 0.0211 - val_loss: 0.0125\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 126s 227ms/step - loss: 0.0197 - val_loss: 0.0138\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 117s 210ms/step - loss: 0.0183 - val_loss: 0.0122\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 132s 237ms/step - loss: 0.0162 - val_loss: 0.0118\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 130s 233ms/step - loss: 0.0165 - val_loss: 0.0113\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 130s 233ms/step - loss: 0.0154 - val_loss: 0.0123\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 120s 217ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 127s 228ms/step - loss: 0.0149 - val_loss: 0.0105\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 121s 217ms/step - loss: 0.0140 - val_loss: 0.0123\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 116s 209ms/step - loss: 0.0128 - val_loss: 0.0109\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 117s 210ms/step - loss: 0.0132 - val_loss: 0.0113\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 117s 210ms/step - loss: 0.0131 - val_loss: 0.0110\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 120s 216ms/step - loss: 0.0121 - val_loss: 0.0105\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 127s 229ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 122s 219ms/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 121s 218ms/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 123s 220ms/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 113s 203ms/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 103s 185ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 95s 171ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 94s 168ms/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 95s 171ms/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 94s 169ms/step - loss: 0.0090 - val_loss: 0.0111\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 92s 166ms/step - loss: 0.0090 - val_loss: 0.0107\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 91s 164ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 94s 169ms/step - loss: 0.0090 - val_loss: 0.0096\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 94s 169ms/step - loss: 0.0081 - val_loss: 0.0099\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 94s 170ms/step - loss: 0.0081 - val_loss: 0.0097\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 92s 165ms/step - loss: 0.0083 - val_loss: 0.0097\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 92s 165ms/step - loss: 0.0074 - val_loss: 0.0099\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 92s 166ms/step - loss: 0.0077 - val_loss: 0.0095\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 92s 164ms/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 93s 168ms/step - loss: 0.0070 - val_loss: 0.0103\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 91s 164ms/step - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 91s 164ms/step - loss: 0.0076 - val_loss: 0.0094\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 93s 167ms/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - ETA: 0s - loss: 0.0067Restoring model weights from the end of the best epoch: 21.\n",
      "556/556 [==============================] - 92s 166ms/step - loss: 0.0067 - val_loss: 0.0107\n",
      "Epoch 41: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0015,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[es],\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6I2vJFiiTkQE",
    "outputId": "83815ac0-1917-4508-b093-bbf2ce91ea08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename  file type\n",
      "0        1.png     1  png\n",
      "1        2.png     2  png\n",
      "2        3.png     3  png\n",
      "3        4.png     4  png\n",
      "4        5.png     5  png\n",
      "...        ...   ...  ...\n",
      "1015  1016.png  1016  png\n",
      "1016  1017.png  1017  png\n",
      "1017  1018.png  1018  png\n",
      "1018  1019.png  1019  png\n",
      "1019  1020.png  1020  png\n",
      "\n",
      "[1020 rows x 3 columns]\n",
      "Found 1020 validated image filenames.\n",
      "<keras.preprocessing.image.DataFrameIterator object at 0x7f71233f10d0>\n",
      "32/32 [==============================] - 5s 106ms/step\n",
      "(1020, 2)\n"
     ]
    }
   ],
   "source": [
    "filename = os.listdir(\"machine-learning-in-science-ii-2023/test_data/test_data\") \n",
    "\n",
    "df = pd.DataFrame(filename)\n",
    "df.columns = [\"filename\"]\n",
    "\n",
    "df[['file', 'type']] = df.filename.str.split(\".\", expand = True)\n",
    "df[\"file\"] = df[\"file\"].astype(str).astype(int)\n",
    "\n",
    "df.sort_values(by=['file'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_images = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=\"machine-learning-in-science-ii-2023/test_data/test_data\",\n",
    "    x_col=\"filename\",\n",
    "    target_size=img_size,\n",
    "    shuffle=False,\n",
    "    class_mode=None)\n",
    "\n",
    "print(test_images)\n",
    "\n",
    "prediction = model.predict(test_images)\n",
    "print(prediction.shape)\n",
    "import math\n",
    "\n",
    "a= prediction[:,0]\n",
    "b= prediction[:,1]\n",
    "b+=0.5\n",
    "b= np.floor(b)\n",
    "\n",
    "submissiondata={\"angle\":a , \"speed\":b} \n",
    "df = pd.DataFrame(submissiondata)\n",
    "df.index += 1 \n",
    "df.to_csv('Submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 10s 103ms/step - loss: 0.0090\n",
      "Found 2966 validated image filenames.\n",
      "93/93 [==============================] - 6s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_loss = model.evaluate(\n",
    "    eval_generator,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=x_evaluate,\n",
    "    directory=\"machine-learning-in-science-ii-2023/training_data/training_data\",\n",
    "    x_col=\"filename\",\n",
    "    target_size=img_size,\n",
    "    shuffle=False,\n",
    "    class_mode=None)\n",
    "\n",
    "\n",
    "prediction = model.predict(test_generator, verbose =1)\n",
    "\n",
    "a= prediction[:,0]\n",
    "b= prediction[:,1]\n",
    "b+=0.5\n",
    "b= np.floor(b)\n",
    "\n",
    "submissiondata={\"angle\":a , \"speed\":b} \n",
    "df = pd.DataFrame(submissiondata)\n",
    "df.index += 1 \n",
    "df.to_csv('evaluate.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11860</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.631705</td>\n",
       "      <td>0.069205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11861</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.429378</td>\n",
       "      <td>0.195622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.777185</td>\n",
       "      <td>0.160315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.746991</td>\n",
       "      <td>0.065509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>0.041818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.392814</td>\n",
       "      <td>0.169686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14822</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.640609</td>\n",
       "      <td>0.015609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14823</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.710362</td>\n",
       "      <td>0.022862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14824</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.474212</td>\n",
       "      <td>0.025788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14825</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.616205</td>\n",
       "      <td>0.008795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2966 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Train  Predicted  Difference\n",
       "11860  0.5625   0.631705    0.069205\n",
       "11861  0.6250   0.429378    0.195622\n",
       "11862  0.9375   0.777185    0.160315\n",
       "11863  0.8125   0.746991    0.065509\n",
       "11864  0.4375   0.479318    0.041818\n",
       "...       ...        ...         ...\n",
       "14821  0.5625   0.392814    0.169686\n",
       "14822  0.6250   0.640609    0.015609\n",
       "14823  0.6875   0.710362    0.022862\n",
       "14824  0.5000   0.474212    0.025788\n",
       "14825  0.6250   0.616205    0.008795\n",
       "\n",
       "[2966 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = abs(x_evaluate[\"angle\"] - a)\n",
    "\n",
    "results = pd.DataFrame({\"Train\":x_evaluate[\"angle\"], \"Predicted\":a, \"Difference\":diff})\n",
    "results\n",
    "\n",
    "#results.to_csv('evaluate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11860</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11861</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14823</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14825</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2966 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train  Predicted\n",
       "11860    1.0        1.0\n",
       "11861    1.0        0.0\n",
       "11862    1.0        1.0\n",
       "11863    1.0        1.0\n",
       "11864    1.0        1.0\n",
       "...      ...        ...\n",
       "14821    1.0        1.0\n",
       "14822    1.0        1.0\n",
       "14823    1.0        1.0\n",
       "14824    1.0        1.0\n",
       "14825    1.0        1.0\n",
       "\n",
       "[2966 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Train\":x_evaluate[\"speed\"],\"Predicted\":b})\n",
    "results\n",
    "\n",
    "#results.to_csv('evaluate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrain_model_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel compiled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m loaded_model\n\u001b[0;32m---> 23\u001b[0m model_config_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mE:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mShelfie\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mShelfie-Algorithm\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mML_Algorithm_Stack\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mML_Models\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCNN_Models\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mModel_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodel_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_config.yaml\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(retrain_model_ID, model_type, retrain_model_ID)\n\u001b[1;32m     24\u001b[0m \u001b[39m# sub_model_weights_file = \"E:\\\\Shelfie\\\\Shelfie-Algorithm\\\\ML_Algorithm_Stack\\ML_Models\\\\CNN_Models\\\\Model_{}_CNN_GA\\\\Model_{}_Checkpoints\\\\30R29_weights_26-0.0517-0.9803-0.1515-0.9568.hdf5\".format(retrain_model_ID, retrain_model_ID)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sub_model_weights_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mE:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mShelfie\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mShelfie-Algorithm\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mML_Algorithm_Stack\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mML_Models\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCNN_Models\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mModel_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39mModel_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_Checkpoints\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m60_weights_07-0.1071-0.9635-0.1447-0.9559.hdf5\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(retrain_model_ID, model_type, retrain_model_ID)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retrain_model_ID' is not defined"
     ]
    }
   ],
   "source": [
    "def get_ML_model(model_config_path, model_weights_path, _loss_='categorical_crossentropy', _optimizer_='adam', _metrics_='accuracy'):\n",
    "    # load YAML and create model\n",
    "    yaml_file = open(model_config_path, 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_weights_path)\n",
    "    print(\"Model_weights_loaded\")\n",
    "    # evaluate loaded model on test data\n",
    "\n",
    "    loaded_model.compile(\n",
    "\tloss=_loss_, \n",
    "\toptimizer=_optimizer_, \n",
    "\tmetrics=[_metrics_])    \n",
    "\n",
    "    print(\"Model compiled\")\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "model_config_file = \"E:\\\\Shelfie\\\\Shelfie-Algorithm\\\\ML_Algorithm_Stack\\ML_Models\\\\CNN_Models\\\\Model_{}_{}\\\\model_{}_config.yaml\".format(retrain_model_ID, model_type, retrain_model_ID)\n",
    "# sub_model_weights_file = \"E:\\\\Shelfie\\\\Shelfie-Algorithm\\\\ML_Algorithm_Stack\\ML_Models\\\\CNN_Models\\\\Model_{}_CNN_GA\\\\Model_{}_Checkpoints\\\\30R29_weights_26-0.0517-0.9803-0.1515-0.9568.hdf5\".format(retrain_model_ID, retrain_model_ID)\n",
    "sub_model_weights_file = \"E:\\\\Shelfie\\\\Shelfie-Algorithm\\\\ML_Algorithm_Stack\\ML_Models\\\\CNN_Models\\\\Model_{}_{}\\\\Model_{}_Checkpoints\\\\60_weights_07-0.1071-0.9635-0.1447-0.9559.hdf5\".format(retrain_model_ID, model_type, retrain_model_ID)\n",
    "model = get_ML_model(model_config_file, sub_model_weights_file)\n",
    "epoch_start = print(\"Model loaded from epoch {} - {}\".format(epoch_start, sub_model_weights_file))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=total_train_samples / BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validation_samples / BATCH_SIZE,\n",
    "    initial_epoch=epoch_start,\n",
    "    callbacks=[csv_logger, check_point, early_stopping, tensorboard])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"model.h5\")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0015,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[es],\n",
    "    epochs=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlis2_car",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "83badb9abfa833c8f944a253dee1819f5989bd8424f7fb8e0e95b6cecc08b674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
