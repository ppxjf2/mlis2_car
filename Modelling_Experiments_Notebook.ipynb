{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1778398d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "import yaml\n",
    "from IPython.core.display import SVG\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model, model_to_dot\n",
    "\n",
    "import sakthi_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becfd59d-7760-4ec1-8bb4-a7ef5a1076df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccfb75c-f0a9-4297-bb53-03c5d34f7ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset_path = \"data/all_data\"\n",
    "test_dataset_path = \"machine-learning-in-science-ii-2023/test_data/test_data\"\n",
    "\n",
    "training_labels_path = \"data/cleaned_training_norm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb761141-5592-4312-9111-bbf151097d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(13792, 3)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(training_labels_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   image_id   angle  speed\n0         1  0.4375    0.0\n1         2  0.8125    1.0\n2         3  0.4375    1.0\n3         4  0.6250    1.0\n4         5  0.5000    0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>angle</th>\n      <th>speed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.4375</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.8125</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.4375</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.6250</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.5000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584685af-ba7c-41ff-a5d7-e9403c378b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   image_id   angle  speed file_name\n0         1  0.4375    0.0     1.png\n1         2  0.8125    1.0     2.png\n2         3  0.4375    1.0     3.png\n3         4  0.6250    1.0     4.png\n4         5  0.5000    0.0     5.png",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>angle</th>\n      <th>speed</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.4375</td>\n      <td>0.0</td>\n      <td>1.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.8125</td>\n      <td>1.0</td>\n      <td>2.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.4375</td>\n      <td>1.0</td>\n      <td>3.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.6250</td>\n      <td>1.0</td>\n      <td>4.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.5000</td>\n      <td>0.0</td>\n      <td>5.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"file_name\"] = df[\"image_id\"].astype(str) + \".png\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Test Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251ae887-2183-466c-8b39-314cadd466ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11033, 4) 11033\n",
      "(2759, 4) 2759\n"
     ]
    }
   ],
   "source": [
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=39, stratify=df[[\"speed\", \"angle\"]])\n",
    "print(train_df.shape, len(train_df[\"image_id\"].unique()))\n",
    "print(validation_df.shape, len(validation_df[\"image_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fe86aa-fbd4-49ea-9e78-4cc35f4a5dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  image_id file_name\n0        1     1.png\n1        2     2.png\n2        3     3.png\n3        4     4.png\n4        5     5.png",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(test_dataset_path)\n",
    "test_image_id_list = []\n",
    "for file in files:\n",
    "    image_id = file.split(\".\")[0]\n",
    "    test_image_id_list.append(image_id)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"image_id\"]=sorted(test_image_id_list, key=sakthi_helper.natural_keys)\n",
    "test_df[\"file_name\"] = df[\"image_id\"].astype(str) + \".png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  image_id file_name\n0        1     1.png\n1        2     2.png\n2        3     3.png\n3        4     4.png\n4        5     5.png",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.to_csv(\"submissions/test_submission_template.csv\", index=False)\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335d119a-f9a8-40e7-a26e-e454d6c6f017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CLASSES = [2, 17]  # CHANGE HERE, total number of classes               \n",
    "IMG_HEIGHT = 240  # CHANGE HERE, the image height to be resized to  \n",
    "IMG_WIDTH = 320  # CHANGE HERE, the image width to be resized to     \n",
    "CHANNELS = 3  # The 3 color channels, change to 1 if grayscale\n",
    "COLOR_MODE = \"rgb\"\n",
    "BATCH_SIZE = 16\n",
    "SEED = 39\n",
    "AUGMENTATION = \"b-0.5-1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Generators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "407a04b5-2cef-4fd4-b6ce-d1d6d547e982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"machine-learning-in-science-ii-2023/training_data/training_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ca7dd6-8983-4903-8c98-a240aaa4ce48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1 / 255, horizontal_flip=False , brightness_range=(0.5, 1.5), zca_whitening=False)\n",
    "                                            # contrast_stretching=True, contrast_stretching_range=(2, 98),\n",
    "                                            # histogram_equalization=True ,\n",
    "                                            # adaptive_equalization=True, adaptive_hist_clip_limit=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e393d3f3-5b38-4c06-ad92-9b8296379056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11033 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator_images_path = \"data/train_generator_images_{}\".format(AUGMENTATION) # hf_b_cs_he_ahe_zca\n",
    "pathlib.Path(train_generator_images_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_generator = train_data_gen.flow_from_dataframe(train_df, directory=dataset_path, x_col='file_name', y_col=[\"speed\", \"angle\"], weight_col=None, \n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=COLOR_MODE, classes=None, class_mode='multi_output', \n",
    "                                                     batch_size=BATCH_SIZE, shuffle=True, seed=SEED,\n",
    "                                                     save_to_dir=train_generator_images_path, save_prefix='Aug', save_format='png',\n",
    "                                                     subset=None, interpolation='nearest', validate_filenames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5cb05fe-ab75-49cd-92d8-21a518acd81e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2759 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow_from_dataframe(validation_df, directory=dataset_path, x_col='file_name', y_col=[\"speed\", \"angle\"], weight_col=None, \n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=COLOR_MODE, classes=None, class_mode='multi_output', \n",
    "                                                     batch_size=BATCH_SIZE, shuffle=True, seed=SEED,\n",
    "                                                     save_to_dir=None, save_prefix='', save_format='png',\n",
    "                                                     subset=None, interpolation='nearest', validate_filenames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "449788b0-0bee-44b1-aa0e-1162e1d3ad20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1020 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_dataframe(test_df, directory=test_dataset_path, x_col='file_name', y_col=None, weight_col=None, \n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=COLOR_MODE, classes=None, class_mode=None, \n",
    "                                                     batch_size=BATCH_SIZE, shuffle=False, seed=SEED,\n",
    "                                                     save_to_dir=None, save_prefix='', save_format='png',\n",
    "                                                     subset=None, interpolation='nearest', validate_filenames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9301e2b6-81be-489e-a4fc-6a6b4f310da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {}\n",
    "\n",
    "model_ID = model_config['model_id'] = \"1\"\n",
    "model_type = model_config['model_type'] = 'CNN_BC_MC'\n",
    "class_names = model_config['class_names_short'] = '' ##[[0, 1], []]\n",
    "class_names_short = model_config['model_id'] = '' #['OA', 'G', 'AG', 'HA', 'PG', 'CBG', 'SEG', 'MBG', 'MBAG', 'WOOS']\n",
    "class_names_dict = model_config['class_names_dict'] = '' ##{0: 'OA', 1: 'G', 2: 'AG', 3: 'HA', 4: 'PG', 5: 'CBG', 6: 'SEG', 7: 'MBG', 8: 'MBAG', 9: 'WOOS'}\n",
    "dataset_ID = model_config['dataset_ID'] = \"INITIAL_1\"\n",
    "seed = model_config['seed'] = SEED\n",
    "n_classes = model_config['n_classes'] = len(model_config['class_names'])\n",
    "class_weights = model_config['class_weights'] = '' ##{0:1, 1:1 , 2:3, 3:5, 4:5, 5:1}\n",
    "batch_size = model_config['batch_size'] = BATCH_SIZE\n",
    "n_epochs = model_config['n_epochs'] = 20\n",
    "hidden_layers = model_config['hidden_layers'] = ['C32_K5', 'C32_K3', 'BN', 'MP_2', 'C64_K5', 'C64_K3', 'BN', 'MP_2', 'C128_K3', 'F', '1024', '128']\n",
    "activation_fns = model_config['activation_fns'] = ['relu', 'softmax']\n",
    "n_layers = model_config['n_layers'] = 0\n",
    "\n",
    "optimizer = model_config['optimizer'] = \"adam\"\n",
    "initializer = model_config['initializer'] = \"glorot_uniform\"\n",
    "loss = model_config['loss'] = \"categorical_crossentropy\"\n",
    "loss_short = model_config['loss_short'] = \"cce\"\n",
    "metrics = model_config['metrics'] = \"accuracy\"\n",
    "preprocess = model_config['preprocess'] = \"normalize_rescale-[0,1]\"\n",
    "augmentation = model_config['augmentation'] = AUGMENTATION  #\"hf, br_0.4_1.3, zca_w\" #\"-\" # #\n",
    "regularisation = model_config['regularisation'] = \"-\"  #\"1_BN, 1_Dropout_0.3 D\" #\"4_Dropout_0.3 in MP,D\" #\"-\" # # #\"3_Dropout_0.3 in F,D\"  #    #\"6 Batch Norm layers\"   ##  #\"-\"   #\"5_Dropout_0.3 in MP,D\"\n",
    "additional_params = model_config['additional_params'] = \"-\"\n",
    "\n",
    "# ---------- CNN --------------------\n",
    "input_shape = model_config['input_shape'] = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "total_train_samples = model_config['total_train_samples'] = train_df.shape[0]\n",
    "total_validation_samples = model_config['total_validation_samples'] = validation_df.shape[0]\n",
    "train_samples = model_config['train_samples'] = train_df.shape[0]\n",
    "validation_samples = model_config['validation_samples'] = validation_df.shape[0]\n",
    "train_dataset = model_config['train_dataset'] = \"\"\n",
    "validation_dataset = model_config['validation_dataset'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## model save path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9cfd8-41bb-4028-8b53-96c394cf2a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_path = \"models/CNN_models/model_{}_{}\".format(model_ID, model_type)\n",
    "checkpoints_save_path = \"models/CNN_models/model_{}_{}/model_{}_checkpoints\".format(model_ID, model_type, model_ID)\n",
    "pathlib.Path(model_save_path).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(checkpoints_save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the CNN model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bc0c4-839f-42fd-a2c7-c69d2d72be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "model = sakthi_helper.create_CNN_model(input_shape, hidden_layers, pretrained_model=None, num_non_trainable_layers=1,\n",
    "                     output_layer={'BC': [1, 'sigmoid', 'binary_crossentropy'], 'MC': [17, 'softmax', 'categorical_crossentropy']},\n",
    "                     init='normal', optimize='adam', metrics=['accuracy', 'mse'])\n",
    "\n",
    "print(\"Model Summary : \", model.summary())\n",
    "\n",
    "model_parameters = model.count_params()\n",
    "print(\"Model Parameters : \", model_parameters)\n",
    "\n",
    "n_layers = 0\n",
    "for i, layer in enumerate(model.layers):  # base_model.layers\n",
    "    n_layers += 1\n",
    "    print(i, layer.name)\n",
    "print(\"num layers: \", n_layers)\n",
    "\n",
    "model_name =  \"{}_{}_D-{}_A-{}_E-{}_B-{}_PP-{}_R-{}_EXTRA-{}\".format(model_ID, model_type, dataset_ID, augmentation, n_epochs, batch_size, preprocess,\n",
    "                                                                      regularisation, additional_params)\n",
    "\n",
    "print(\"Model_name   : \", model_name, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e41145-914e-4e52-a0ec-a2bfab0dbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=model_save_path + \"\\\\{}_plot.png\".format(model_name))\n",
    "print(\"model_plotted\")\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d1b4f-2f3b-4c6b-9078-eec58de85c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------   SAVE MODEL CONFIGURATION  -------------------------------------------\n",
    "\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "\n",
    "model_config_path = model_save_path\n",
    "pathlib.Path(model_config_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Model_config_path : \", model_config_path)\n",
    "\n",
    "model_config_file = model_config_path + \"\\\\model_{}_config.yaml\".format(model_ID)\n",
    "\n",
    "with open(model_config_file, \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "    print(\"Model_config_saved in \\n {}\".format(model_config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c9f1-869b-4b14-88cf-8ff1144fdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------  SAVE MODEL DETAILS  -------------------------------------------------\n",
    "\n",
    "model_info_file = model_save_path + \"\\\\model_{}_build_details.yaml\".format(model_ID)\n",
    "\n",
    "with open(model_info_file, 'w') as outfile:\n",
    "    yaml.dump(model_config, outfile, default_flow_style=False, indent=4)\n",
    "    print(\"model_info_file saved in \\n {}\".format(model_info_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b9ed0-302d-494c-b40f-ca869a1e2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------  CSV DATA  ----------------------------------------------------\n",
    "\n",
    "header_list = []\n",
    "header_list.append('Model ID')\n",
    "header_list.append('Model Type')\n",
    "header_list.append('Dataset ID')\n",
    "header_list.append('Total train Samples')\n",
    "header_list.append('Total Validation samples')\n",
    "header_list.append('Train Samples')\n",
    "header_list.append('Validation Samples')\n",
    "header_list.append('Seed')\n",
    "header_list.append('Input shape')\n",
    "header_list.append('N classes')\n",
    "header_list.append('Class names')\n",
    "header_list.append('Class weights')\n",
    "header_list.append('Preprocessing')\n",
    "header_list.append('Augmentation')\n",
    "header_list.append('N layers')\n",
    "header_list.append('Architecture')\n",
    "header_list.append('Model Parameters')\n",
    "header_list.append('Initializer')\n",
    "header_list.append('Optimizer')\n",
    "header_list.append('Loss function')\n",
    "header_list.append('Regularisation')\n",
    "header_list.append('Metrics')\n",
    "header_list.append('Batch size')\n",
    "header_list.append('N epochs')\n",
    "\n",
    "header_list.append('Train loss')\n",
    "header_list.append('Train accuracy')\n",
    "header_list.append('Test loss')\n",
    "header_list.append('Test accuracy')\n",
    "header_list.append('Sub Model ID')\n",
    "header_list.append('Validation loss')\n",
    "header_list.append('Validation accuracy')\n",
    "header_list.append('Additional params')\n",
    "header_list.append('Remarks')\n",
    "\n",
    "model_details = []\n",
    "model_details.append(model_ID)\n",
    "model_details.append(model_type)\n",
    "model_details.append(dataset_ID)\n",
    "model_details.append(total_train_samples)\n",
    "model_details.append(total_validation_samples)\n",
    "model_details.append(train_samples)\n",
    "model_details.append(validation_samples)\n",
    "model_details.append(seed)\n",
    "model_details.append(input_shape)\n",
    "model_details.append(n_classes)\n",
    "model_details.append(class_names)\n",
    "model_details.append(class_weights)\n",
    "model_details.append(preprocess)\n",
    "model_details.append(augmentation)\n",
    "model_details.append(n_layers)\n",
    "model_details.append(hidden_layers)\n",
    "model_details.append(model_parameters)\n",
    "model_details.append(initializer)\n",
    "model_details.append(optimizer)\n",
    "model_details.append(loss_short)\n",
    "model_details.append(regularisation)\n",
    "model_details.append(metrics)\n",
    "model_details.append(batch_size)\n",
    "model_details.append(n_epochs)\n",
    "\n",
    "print(\"Length of header list : \", len(header_list))\n",
    "print(\"Length of model details_list : \", len(model_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d494152-50ea-4947-bf3c-8e65749947bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------  CALLBACKS  --------------------------------------------------------\n",
    "\n",
    "# metrics_1 = Metrics()\n",
    "\n",
    "#------------  CHECKPOINTS  -----------------------\n",
    "\n",
    "model_metrics = \"_{epoch:02d}-{loss:.4f}-{acc:.4f}-{val_loss:.4f}-{val_acc:.4f}\"\n",
    "\n",
    "filepath = checkpoints_save_path + \"\\\\{}_weights\".format(model_ID) + model_metrics + \".hdf5\"\n",
    "check_point = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#-------------  CSV LOGGER  ----------------------\n",
    "\n",
    "# CSVLogger(filename, separator=',', append=False)\n",
    "csv_log_path = model_save_path + '\\\\model_{}_training_{}.log'.format(model_ID, int(time.time()))\n",
    "csv_logger = CSVLogger(csv_log_path, append=False)\n",
    "\n",
    "#-------------  EARLY STOPPING  --------------------\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20)\n",
    "\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4,\n",
    "                                   # mode='min')\n",
    "\n",
    "#-------------  TENSORBOARD  ------------------------\n",
    "\n",
    "NAME = \"model_{}_logs_{}\".format(model_ID, int(time.time()))\n",
    "tensorboard_path = model_save_path + \"\\\\logs\\\\{}\".format(NAME)\n",
    "pathlib.Path(tensorboard_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dcfc0-eda4-4be6-9182-fdc4299e3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                                      epochs=n_epochs,\n",
    "                                      steps_per_epoch= total_train_samples/BATCH_SIZE,\n",
    "                                      verbose=1, # class_weight=class_weights,\n",
    "                                      validation_data= validation_generator,\n",
    "                                      validation_steps= total_validation_samples/BATCH_SIZE,\n",
    "                                      callbacks=[csv_logger, check_point, early_stopping, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662182ab-b028-47d4-be66-d2d7c6b6b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------  MODEL TRAINING HISTORY  ----------------------------------------------------\n",
    "\n",
    "hist = [history.history]\n",
    "print(hist, end=\"\\n\\n\")\n",
    "# print(hist[0]['acc'][-1])\n",
    "train_loss = hist[0]['loss'][-1]\n",
    "train_accuracy = hist[0]['acc'][-1]\n",
    "val_loss = hist[0]['val_loss'][-1]\n",
    "val_accuracy = hist[0]['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d3f3c-218c-44c9-88a9-9bb17f53e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------  SAVING MODEL  ------------------------------------------------------\n",
    "\n",
    "end_epoch = len(hist[0]['loss'])\n",
    "\n",
    "model_stats = \"{:.4f}_{:.4f}_{:.4f}-{:.4f}\".format(train_loss ,train_accuracy ,val_loss ,val_accuracy)\n",
    "\n",
    "model.save(model_save_path + \"\\\\{}_end_epoch_{}_{}.h5\".format(model_ID, end_epoch, model_stats))\n",
    "print(\"model saved in \\n {}\".format(model_save_path))\n",
    "\n",
    "# serialize weights to HDF5\n",
    "\n",
    "model.save_weights(model_save_path + \"\\\\model_{}_end_epoch_{}_weights_{}.h5\".format(model_ID, end_epoch, model_stats))\n",
    "print(\"model weights saved in \\n {}\".format(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f6c9-c52f-40e1-b7a4-ccee88f56c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------  SAVE PICKLE FILE  -----------------------------------------------\n",
    "\n",
    "joblib.dump(model, model_save_path + \"\\\\model_{}_end_epoch_{}_weights_{}.pickle\".format(model_ID, end_epoch, model_stats))\n",
    "\n",
    "#-----------------------------------  PLOT TRAINING HISTORY  --------------------------------------------\n",
    "\n",
    "history_plot_save_path = model_save_path + \"\\\\{}_Training_Stats.png\".format(model_name)\n",
    "\n",
    "sakthi_helper.plot_histories(hist, history_plot_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f1dc8-0063-47df-88b1-b3708dbf0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"51F47\"\n",
    "\n",
    "model_type = \"CNN_BC_MC\"  # CNN_C_R   # CNN_R\n",
    "\n",
    "epoch_ID = 14  #51F47_weights_14-0.0417-0.9856-0.1764-0.9602\n",
    "\n",
    "dataset_ID = \"3CDR_3\" #\"\"CGA_18\"\n",
    "\n",
    "model_save_path = \"models/CNN_models/model_{}_{}\".format(model_ID, model_type)\n",
    "checkpoints_save_path = \"models/CNN_models/model_{}_{}/Model_{}_Checkpoints\".format(model_ID, model_type, model_ID)\n",
    "\n",
    "model_config_file = model_save_path + \"\\\\model_{}_config.yaml\".format(model_ID)\n",
    "\n",
    "weight_files = os.listdir(checkpoints_save_path)\n",
    "print(weight_files)\n",
    "\n",
    "for i, file in enumerate(sorted(weight_files)):\n",
    "\n",
    "    # if i == min_val_loss_index or i == max_val_acc_index:\n",
    "    # print(file)\n",
    "    epoch_num = file.split(\"_\")[-1].split(\"-\")[0]\n",
    "    # print(\"Epoch num : \", epoch_num)\n",
    "\n",
    "    if int(epoch_num) == epoch_ID:\n",
    "        print(file)\n",
    "\n",
    "        weight_file_name = file\n",
    "\n",
    "\n",
    "## weight_file_name = \"{}_weights_16-0.0379-0.9864-0.1782-0.9571.hdf5\".format(model_ID)\n",
    "\n",
    "weight_stats = weight_file_name.split(\"_\")[-1]\n",
    "epoch_name = (weight_stats).split(\"-\")[0]\n",
    "print(\"Model loaded from Epoch : {}\".format(epoch_name))\n",
    "sub_model_ID = int(epoch_name)\n",
    "\n",
    "#--------  LOADING MODEL WEIGHTS  ----------------\n",
    "selected_model_weights_file = checkpoints_save_path + \"\\\\\" + weight_file_name # .format(model_ID, n_epochs)\n",
    "\n",
    "src_file = selected_model_weights_file\n",
    "\n",
    "sub_model_save_path = model_save_path + \"\\\\{}_SubModel_{}_Val_Dataset_{}\\\\\".format(model_ID, sub_model_ID, dataset_ID)\n",
    "pathlib.Path(sub_model_save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dest_file = sub_model_save_path + weight_file_name\n",
    "\n",
    "if not os.path.isfile(dest_file):\n",
    "    shutil.copy2(src_file, dest_file)\n",
    "else:\n",
    "    print(\"Weight file already exists in Model directory\")\n",
    "\n",
    "sub_model_weights_file = dest_file\n",
    "\n",
    "model_config_file = model_save_path + \"\\\\model_{}_config.yaml\".format(model_ID)\n",
    "\n",
    "loaded_model = sakthi_helper.get_ML_model(model_config_file, sub_model_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7329f91-7780-4509-aa28-453f066b8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------  MODEL EVALUATION  ---------------------------------------------------\n",
    "\n",
    "loss_and_metrics = loaded_model.evaluate_generator(validation_generator,\n",
    "                                                   steps=total_validation_samples / BATCH_SIZE, verbose=1)\n",
    "\n",
    "print(\"Loss and Metrics : \", loss_and_metrics)\n",
    "\n",
    "eval_loss = round(loss_and_metrics[0], ndigits=4)\n",
    "eval_accuracy = round(loss_and_metrics[1] * 100 , ndigits=2)\n",
    "\n",
    "print(\"{}: {:.4f}\".format(loaded_model.metrics_names[0], eval_loss))\n",
    "print(\"{}: {:.2f}\".format(loaded_model.metrics_names[1], eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b989929-686b-4dc0-8de2-30cf4db2b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------  ML EXPERIMENT DOCUMENTATION  -----------------------------------------------------\n",
    "\n",
    "train_loss = float(weight_stats.split(\"-\")[1])\n",
    "train_accuracy = float(weight_stats.split(\"-\")[2]) * 100\n",
    "validation_loss = eval_loss\n",
    "validation_accuracy = eval_accuracy\n",
    "test_loss = '-'\n",
    "test_accuracy = '-'\n",
    "\n",
    "model_details.append(train_loss)\n",
    "model_details.append(train_accuracy)\n",
    "model_details.append(test_loss)\n",
    "model_details.append(test_accuracy)\n",
    "model_details.append(sub_model_ID)\n",
    "model_details.append(validation_loss)\n",
    "model_details.append(validation_accuracy)\n",
    "\n",
    "print(train_loss)\n",
    "print(train_accuracy)\n",
    "print(test_loss)\n",
    "print(test_accuracy)\n",
    "print(validation_loss)\n",
    "print(validation_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "model_details.append(model_config['additional_params'])\n",
    "\n",
    "remarks = \"-\"\n",
    "model_details.append(\"-\")\n",
    "\n",
    "print(\"-----------\")\n",
    "# print(len(header_list))\n",
    "print(len(model_details))\n",
    "\n",
    "print(model_details)\n",
    "\n",
    "ML_models_path = \"models/CNN_models/\"\n",
    "CNN_model_results_file = \"\\\\CNN_model_results_New.csv\"\n",
    "\n",
    "CNN_model_results_csv_path = ML_models_path + CNN_model_results_file\n",
    "\n",
    "\n",
    "with open(CNN_model_results_csv_path, 'a', newline='') as file:\n",
    "    file_empty = os.stat(CNN_model_results_csv_path).st_size == 0\n",
    "    csv_writer = csv.writer(file, lineterminator=\"\\n\")\n",
    "\n",
    "    if file_empty:\n",
    "        csv_writer.writerow(header_list)\n",
    "    csv_writer.writerow(model_details)\n",
    "    print(\"Written Experiment stats to csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7c21b-768b-4cdc-9b74-14125696e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----------------------- INFERENCE --------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7571b-24b5-4bf1-930b-f6ae7de83dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------  PLOTING CONFUSION MATRIX  ----------------------------------------\n",
    "# print(Y_pred_list)\n",
    "# print(Y_test_list)\n",
    "#\n",
    "# print(\"\\n\", pred_val_list)\n",
    "# print(y_val_list)\n",
    "\n",
    "cnf_matrix = sakthi_helper.confusion_matrix(Y_test_list, Y_pred_list)\n",
    "\n",
    "print(cnf_matrix)\n",
    "\n",
    "cnf_plot_save_path = sub_model_save_path + \"\\\\Model_{}-{}_Val_Dataset_{}_cnf_matrix.png\".format(model_ID, sub_model_ID, dataset_ID)\n",
    "\n",
    "sakthi_helper.plot_confusion_matrix(cnf_matrix, class_names, cnf_plot_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d6196-2c80-4ff2-9334-d11214fd837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------  PLOTTING ROC CURVES  ---------------------------------------------\n",
    "\n",
    "roc_plot_save_path_full = sub_model_save_path + \"\\\\Model_{}-{}_Val_Dataset_{}_full_ROC_plot.png\".format(model_ID, sub_model_ID, dataset_ID)\n",
    "roc_plot_save_path_zoom = sub_model_save_path + \"\\\\Model_{}-{}_Val_Dataset_{}_zoomed_ROC_plot.png\".format(model_ID, sub_model_ID, dataset_ID)\n",
    "\n",
    "# print(Y_one_hot_array.shape)\n",
    "# print(Y_pred_one_hot_array.shape)\n",
    "\n",
    "\n",
    "# sakthi_helper.plot_ROC_curves(Y_one_hot_array, Y_pred_one_hot_array, n_classes, roc_plot_save_path_full, roc_plot_save_path_zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d18457-54f7-422f-9109-4eadecea483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------  PLOT CLASSIFICATION REPORT  ----------------------------------\n",
    "\n",
    "report = sakthi_helper.classification_report(Y_test_array, Y_pred_array, target_names=class_names)\n",
    "print(\"Classification Report : \", report)\n",
    "\n",
    "report_plot_save_path = sub_model_save_path + \"\\\\Model_{}-{}_Val_Dataset_{}_Classification_Report.png\".format(model_ID, sub_model_ID, dataset_ID)\n",
    "\n",
    "sakthi_helper.plot_classification_report(report, report_plot_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d73fa8-3a22-4be5-965e-d17804af9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------  SAVE CLASSIFICATION REPORT AS CSV  ------------------------------------\n",
    "\n",
    "report_save_path = sub_model_save_path + \"\\\\Model_{}-{}_Val_Dataset_{}_Classification_Report.csv\".format(model_ID,\n",
    "                                                                                               sub_model_ID, dataset_ID)\n",
    "\n",
    "report_df = sakthi_helper.pandas_classification_report(Y_test_array, Y_pred_array, class_names)\n",
    "\n",
    "print(report_df)\n",
    "\n",
    "report_df.to_csv(report_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8466233-7532-4da8-bf26-627b15798799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4553a0-710b-47b7-b4b4-737e408dedf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py39",
   "language": "python",
   "display_name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
